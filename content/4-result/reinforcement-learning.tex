
En esta sección demostramos la aplicación práctica de los conceptos de aprendizaje por refuerzo
(Sección~\ref{subsec:rl-components}) al problema de estacionamiento automático, utilizando
los resultados de detección de retícula obtenidos mediante las técnicas de visión por computadora
desarrolladas en este trabajo.

\subsection{Configuración del entorno con Gymnasium y CARLA}\label{subsec:rl-environment}

Implementamos el entorno de aprendizaje por refuerzo utilizando \texttt{Gymnasium} como framework
estándar para la interfaz agente-entorno, integrado con el simulador CARLA que proporciona
un mundo físicamente realista. 

CARLA simula las leyes de la física vehicular con alta fidelidad, incluyendo la dinámica de carrocería, 
fricción de neumáticos y colisiones realistas en el contexto de un estacionamiento con cajones delimitados. 
El vehículo simulado está equipado con sensores que replican los de un vehículo real: cámara frontal, 
velocímetro y sensores de pose. Por su parte, Gymnasium proporciona la API estándar que permite 
intercambiar diferentes algoritmos de aprendizaje por refuerzo sin necesidad de modificar el código 
del entorno, facilitando la experimentación con distintos enfoques de entrenamiento.

\subsection{Definición del agente y espacio de acciones}\label{subsec:rl-agent}

El agente implementa una política que mapea observaciones a acciones de control vehicular.
Siguiendo la terminología de la Sección~\ref{subsec:rl-components}, definimos:


\textbf{Espacio de acciones}: Utilizamos un espacio continuo bidimensional
$\mathcal{A} = \text{Box}([-1, -1], [1, 1])$ donde cada acción $\mathbf{a} = [a_{\text{acc}}, a_{\text{dir}}]$ representa:
\begin{itemize}
    \item $a_{\text{acc}} \in [-1, 1]$: Control de aceleración. Valores positivos corresponden
    a aceleración hacia adelante, negativos a reversa, y cero a frenado.
    \item $a_{\text{dir}} \in [-1, 1]$: Control de dirección. Valores negativos giran a la izquierda,
    positivos a la derecha, y cero mantiene la dirección recta.
\end{itemize}

\subsection{Espacio de observaciones y sistema intérprete}\label{subsec:rl-observations}

El estado del sistema se representa mediante un vector de observación $\mathbf{s} \in \mathbb{R}^6$:
\begin{equation}
    \mathbf{s} = [x, y, v_x, v_y, \cos(\theta), \sin(\theta)]
\end{equation}

donde:
\begin{itemize}
    \item $(x, y)$: Posición del vehículo en coordenadas del estacionamiento
    \item $(v_x, v_y)$: Componentes de velocidad
    \item $(\cos(\theta), \sin(\theta))$: Orientación del vehículo (representación continua del ángulo)
\end{itemize}


\textbf{Sistema intérprete}: La estimación de posición y orientación $(x, y, \theta)$ se obtiene
aplicando los procedimientos de visión por computadora desarrollados en este trabajo:
detección de la retícula de estacionamiento (Sección~\ref{sec:metodo-reticula}) y
su ajuste mediante RANSAC (Sección~\ref{sec:metodo-ransac}). La velocidad se obtiene
directamente del sensor velocímetro del vehículo simulado.

\subsection{Adaptación del framework Highway-env}\label{subsec:rl-highway-adaptation}

Para la implementación del aprendizaje por refuerzo adoptamos el framework Highway-env \cite{highway-env}, (véase Sección~\ref{subsec:rl-highway-theory}) una biblioteca especializada en entornos de conducción autónoma que proporciona implementaciones
probadas y exitosas de espacios de acción, observación y funciones de recompensa para tareas
de estacionamiento. En lugar de desarrollar estos componentes desde cero, utilizamos esta solución 
establecida como plataforma de prueba para validar nuestros algoritmos de visión por computadora 
en un contexto de aplicación real.


\textbf{Adaptación realizada}: Para lograr un entorno más realista y cercano al mundo real, 
reemplazamos el sistema de observación simplificado de Highway-env por nuestro intérprete basado en
los algoritmos de detección de retícula desarrollados en este trabajo, y migramos la
simulación del entorno abstracto original a CARLA para obtener una física vehicular
más precisa. Esta adaptación nos permite evaluar el desempeño de nuestros algoritmos de visión 
en condiciones más cercanas a las de un sistema de estacionamiento automático real.


\textbf{Componentes reutilizados de Highway-env}: Para la implementación del entorno de aprendizaje, 
adoptamos los siguientes elementos del framework original que han demostrado ser efectivos para 
tareas de estacionamiento:

\textbf{Espacio de observaciones}: Vector continuo de 6 elementos $\mathbf{s} \in \mathbb{R}^6$ que representa
el estado completo del vehículo:
\begin{equation}
\mathbf{s} = [x, y, v_x, v_y, \cos(\theta), \sin(\theta)]
\end{equation}
donde $(x, y)$ es la posición en coordenadas del estacionamiento, $(v_x, v_y)$ son las componentes 
de velocidad, y $(\cos(\theta), \sin(\theta))$ representa la orientación del vehículo.

\textbf{Espacio de acciones}: $\mathcal{A} = \text{Box}([-1, -1], [1, 1])$ para control continuo, 
donde cada acción $\mathbf{a} = [a_{\text{acc}}, a_{\text{dir}}]$ controla aceleración y dirección 
respectivamente.

\textbf{Función de recompensa}: La recompensa en cada paso está dada por:
\begin{equation}
r = -|\mathbf{g} - \mathbf{s}| - \alpha \cdot |\mathbf{a}|
\end{equation}
donde $\mathbf{g}$ es la posición objetivo (desired goal), $\mathbf{s}$ es el estado actual 
(achieved goal), $\mathbf{a}$ es la acción tomada, y $\alpha$ es un factor de regularización 
que penaliza acciones extremas para promover maniobras suaves.

\textbf{Objetivo}: El agente debe alcanzar una posición objetivo específica (landmark) con la 
orientación deseada, correspondiente a un cajón de estacionamiento válido dentro de la retícula detectada.

\textbf{Estrategia de episodios}: Se utilizan los criterios de terminación y reinicio del entorno 
originales de Highway-env.


Los detalles completos del diseño original de Highway-env se describen en la
Sección~\ref{subsec:highway-env-theory}. En la siguiente sección analizaremos
la implementación específica y los resultados obtenidos con esta adaptación.

\subsection{Integración de visión por computadora y RL}\label{subsec:rl-integration}

La contribución principal de este trabajo radica en la integración del sistema de
detección de retícula desarrollado como intérprete de observaciones en el framework
de aprendizaje por refuerzo. Mientras que Highway-env utiliza información de pose
perfecta y simplificada, nuestro sistema debe:

\begin{itemize}
    \item \textbf{Procesar imágenes reales}: Aplicar los algoritmos de Canny, Hough y RANSAC
    a la imagen de la cámara frontal del vehículo simulado en CARLA
    \item \textbf{Estimar pose}: Convertir la detección de retícula en estimaciones de
    posición $(x,y)$ y orientación $\theta$ del vehículo respecto al estacionamiento
    \item \textbf{Manejar incertidumbre}: Lidiar con detecciones imperfectas y oclusiones
    variables que no existen en el entorno abstracto original
\end{itemize}


Esta integración permite evaluar la robustez y utilidad práctica de los algoritmos
de visión por computadora en una aplicación de control en tiempo real, demostrando
que es posible reemplazar sensores idealizados por sistemas de percepción basados
en cámara para tareas de navegación autónoma compleja.
